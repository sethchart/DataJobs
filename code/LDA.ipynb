{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following this resource\n",
    "https://www.kdnuggets.com/2019/09/overview-topics-extraction-python-latent-dirichlet-allocation.html\n",
    "\n",
    "https://github.com/FelixChop/MediumArticles/blob/master/LDA-BBC.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Plotting\n",
    "import plotly.express as px\n",
    "\n",
    "# Database\n",
    "from JobsDb import JobsDb\n",
    "\n",
    "# Data Processing\n",
    "from DataProcessor import data_processor\n",
    "\n",
    "# Build corpus\n",
    "from gensim import corpora\n",
    "\n",
    "# Latent Dirichlet Allocation\n",
    "from gensim import models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9485, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>url</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Railcar Verifier/Transload Team Member/Data Entry</td>\n",
       "      <td>https://www.careerjet.com/jobad/us5194732b36a6...</td>\n",
       "      <td>\\nCompany Overview  Come join a Winning Team! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Entry Clerk</td>\n",
       "      <td>https://www.careerjet.com/jobad/us83f88fb60b47...</td>\n",
       "      <td>\\n prepare, compile and sort documents for dat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>https://www.careerjet.com/jobad/us466d6146a815...</td>\n",
       "      <td>\\n  \\n    Data Scientist is responsible for co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Provider Data Specialist</td>\n",
       "      <td>https://www.careerjet.com/jobad/uscb5cda0893f6...</td>\n",
       "      <td>\\n  \\n    Title: Provider Data Specialist  Loc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Security Data Architect</td>\n",
       "      <td>https://www.careerjet.com/jobad/us00dc3c284dbd...</td>\n",
       "      <td>\\nOur Mission  At Dobbs Defense, we deliver mi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title  \\\n",
       "0  Railcar Verifier/Transload Team Member/Data Entry   \n",
       "1                                   Data Entry Clerk   \n",
       "2                                     Data Scientist   \n",
       "3                           Provider Data Specialist   \n",
       "4                            Security Data Architect   \n",
       "\n",
       "                                                 url  \\\n",
       "0  https://www.careerjet.com/jobad/us5194732b36a6...   \n",
       "1  https://www.careerjet.com/jobad/us83f88fb60b47...   \n",
       "2  https://www.careerjet.com/jobad/us466d6146a815...   \n",
       "3  https://www.careerjet.com/jobad/uscb5cda0893f6...   \n",
       "4  https://www.careerjet.com/jobad/us00dc3c284dbd...   \n",
       "\n",
       "                                         description  \n",
       "0  \\nCompany Overview  Come join a Winning Team! ...  \n",
       "1  \\n prepare, compile and sort documents for dat...  \n",
       "2  \\n  \\n    Data Scientist is responsible for co...  \n",
       "3  \\n  \\n    Title: Provider Data Specialist  Loc...  \n",
       "4  \\nOur Mission  At Dobbs Defense, we deliver mi...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = JobsDb()\n",
    "df = db.load_table_as_df('jobs')\n",
    "db.close()\n",
    "df = df.iloc[9680:]\n",
    "data = df.copy()\n",
    "data = data.reset_index().drop(['id', 'index'], axis=1)\n",
    "print(df.shape)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\nCompany Overview  Come join a Winning Team! Since 1970, Plastic Express has been leading the bulk trucking, bulk terminal, packaging, and warehousing needs of the plastics industry. Our strategic locations, modern systems, and dedicated employees allow us to provide custom tailored logistical solutions to fulfill the most challenging needs of our customers. Plastic Express operates from 15 warehouse locations and 37 rail terminals across the US. At many of the Plastic Express sites, we also handle some non-plastic commodities, which include; paper rolls, steel, building materials and other dry bulk materials. Plastic Express owns and operates roughly 130 trucks, with approximately 200 trailers performing full bulk truck distribution business. Plastic Express is headquartered in City of Industry, CA and has over 300 employees nationwide. Our goal has always been to exceed our customers’ expectations, and our “Can Do” attitude is what differentiates us from the competition.  Position Type/Expected Hours of Work  This is a Full Time position. Must be able to work some long hours, fill in sub for full-time employees, weekends and holidays when needed.  Qualifications:  At least 21 years old. Must read, write, and speak English fluently. Must have basic computer skills. Successfully pass a physical exam to assure agility. Must be able and comfortable working outside in all weather conditions. Must be able to walk, bend, reach, push, pull, stoop, squat, and climb. May be required to provide a personal vehicle.  Job Summary:  Operate with safety in mind at all times. Verify railcar connections (BT-18) Data Entry  Duties:  Duties include but are not limited to:  Inspect inbound railcars, verifying physical railcars for all truck entering the railyard. Responsible for all maintenance on the company trucks. Operate in compliance with company, rail yard and FRA safety regulations. Must follow all safety rules Report accidents and incidents involving company or customer assets. Perform additional rail related duties as required. Duties and responsibilities may be added, deleted and or changed at any time. Data entry daily and transload paperwork Keep your office clean and help in all areas of the office where needed   Powered by JazzHR\\\\n  \\\\n  \\\\n  \\\\n    Jazz\\\\n  \\\\n'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = list(data['description'])\n",
    "doc = docs[0]\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_data = data_processor(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_tokenizer(doc):\n",
    "    doc = doc.replace('\\\\n','').lower()\n",
    "    sentences = sent_tokenize(doc)\n",
    "    doc_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
    "    return doc_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time doc_tokens = doc_tokenizer(doc)\n",
    "doc_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parts of Speech Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired from https://stackoverflow.com/a/15590384\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"Converts a treebank POS tag to a wordnet POS tag.\"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        tag = wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        tag = wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        tag = wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        tag = wordnet.ADV\n",
    "    else:\n",
    "        tag = ''\n",
    "    return tag\n",
    "\n",
    "def sentence_pos_tagger(sentence):\n",
    "    \"\"\"Takes a sentence as a list of tokens and returns a list of wordnet POS tagged tokens\"\"\"\n",
    "    treebank_tags = pos_tag(sentence)\n",
    "    wordnet_tags = [ \n",
    "        (treebank_tag[0], get_wordnet_pos(treebank_tag[1])) for treebank_tag in treebank_tags\n",
    "    ]\n",
    "    return wordnet_tags\n",
    "\n",
    "def doc_pos_tagger(doc_tokens):\n",
    "    pos_tags = [\n",
    "        sentence_pos_tagger(sentence) for sentence in doc_tokens\n",
    "    ]\n",
    "    return pos_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time doc_tags = doc_pos_tagger(doc_tokens)\n",
    "doc_tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def tag_lemmatizer(pos_tag):\n",
    "    \"\"\"Lemmatized a POS tagged word.\"\"\"\n",
    "    if pos_tag[1] != '':\n",
    "        lemmatized_word = lemmatizer.lemmatize(pos_tag[0], pos_tag[1])\n",
    "    else:\n",
    "        lemmatized_word = pos_tag[0]\n",
    "    return lemmatized_word\n",
    "    \n",
    "def sentence_lemmatizer(sentence_tags):\n",
    "    \"\"\"Lemmatize POS tagged words from a tagged sentence.\"\"\"\n",
    "    lemmatized_sentence = [\n",
    "        tag_lemmatizer(pos_tag) for pos_tag in sentence_tags\n",
    "    ]\n",
    "    return lemmatized_sentence\n",
    "\n",
    "def doc_lemmatizer(doc_tags):\n",
    "    \"\"\"Lemmetize tagged words from a job doc and flatten sentence nesting.\"\"\"\n",
    "    lemmatized_doc = []\n",
    "    for sentence_tags in doc_tags:\n",
    "        lemmatized_sentence = sentence_lemmatizer(sentence_tags) \n",
    "        lemmatized_doc.extend(lemmatized_sentence)\n",
    "    return lemmatized_doc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time lemmatized_doc = doc_lemmatizer(doc_tags)\n",
    "lemmatized_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing Stopwords and Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_doc(lemmatized_doc):\n",
    "    my_stopwords = stopwords.words('english')\n",
    "    cleaned_doc = [\n",
    "        word for word in lemmatized_doc\n",
    "        if word.isalpha() and word not in my_stopwords\n",
    "        and len(word)>1\n",
    "    ]\n",
    "    return cleaned_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time cleaned_doc = clean_doc(lemmatized_doc)\n",
    "cleaned_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Bigrams and Trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def combine_grams(cleaned_doc):\n",
    "    bigram_model = Phrases(cleaned_doc)\n",
    "    trigram_model = Phrases(bigram_model[cleaned_doc], min_count=1)\n",
    "    processed_doc = list(trigram_model[bigram_model[cleaned_doc]])\n",
    "    return processed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time processed_doc = combine_grams(cleaned_doc)\n",
    "processed_doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_processor(doc):\n",
    "    doc_tokens = doc_tokenizer(doc)\n",
    "    doc_tags = doc_pos_tagger(doc_tokens)\n",
    "    lemmatized_doc = doc_lemmatizer(doc_tags)\n",
    "    cleaned_doc = clean_doc(lemmatized_doc)\n",
    "    processed_doc = combine_grams(cleaned_doc)\n",
    "    return processed_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time processed_doc_2 = doc_processor(doc)\n",
    "assert processed_doc == processed_doc_2, \"Should match\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time processed_docs = [doc_processor(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make token dictionary and corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary_LDA = corpora.Dictionary(processed_docs)\n",
    "dictionary_LDA.filter_extremes(no_below=3)\n",
    "corpus = [dictionary_LDA.doc2bow(token) for token in processed_docs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123456)\n",
    "num_topics = 2\n",
    "alpha = [0.01]*num_topics\n",
    "eta = [0.01]*len(dictionary_LDA.keys())\n",
    "lda_model = models.LdaModel(\n",
    "    corpus, \n",
    "    num_topics=num_topics,\n",
    "    id2word=dictionary_LDA,\n",
    "    passes=4, \n",
    "    alpha=alpha,\n",
    "    eta=eta\n",
    ")\n",
    "lda_model.save(f'../model/lda-{num_topics}topics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspecting Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,topic in lda_model.show_topics(formatted=True,\n",
    "                                     num_topics=num_topics, \n",
    "                                     num_words=20):\n",
    "    print(str(i)+\": \"+ topic)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model[corpus[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://cran.r-project.org/web/packages/LDAvis/vignettes/details.pdf\n",
    "# Here a short legend to explain the vis:\n",
    "# size of bubble: proportional to the proportions of the topics across the\n",
    "# N total tokens in the corpus\n",
    "# red bars: estimated number of times a given term was generated by a given topic\n",
    "# blue bars: overall frequency of each term in the corpus\n",
    "# -- Relevance of words is computed with a parameter lambda\n",
    "# -- Lambda optimal value ~0.6 \n",
    "# (https://nlp.stanford.edu/events/illvi2014/papers/sievert-illvi2014.pdf)\n",
    "%matplotlib inline\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim\n",
    "vis = pyLDAvis.gensim.prepare(\n",
    "    topic_model=lda_model, \n",
    "    corpus=corpus, \n",
    "    dictionary=dictionary_LDA\n",
    ")\n",
    "pyLDAvis.enable_notebook()\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JobDash",
   "language": "python",
   "name": "jobdash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
