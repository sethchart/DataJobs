{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from job_database import JobsDb\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "import inspect\n",
    "from IPython.display import Markdown, display\n",
    "\n",
    "def display_object(obj):\n",
    "    \"\"\"Summary of display_object. Get source code for the provided object \n",
    "    and display in notebook as markdown.\n",
    "    \"\"\"\n",
    "    source = inspect.getsource(obj)\n",
    "    wrapped_source = f'```python\\n{source}\\n```'\n",
    "    markdown_source = Markdown(wrapped_source)\n",
    "    display(markdown_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "SELECT * from jobs\n",
    "WHERE title LIKE '%data%'\n",
    "OR description LIKE '%data%';\n",
    "\"\"\"\n",
    "db = JobsDb()\n",
    "data = db.load_query_as_df(query)\n",
    "db.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "descriptions = df['description'].tail(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    steps = [\n",
    "        ('vectorizer', CountVectorizer(\n",
    "            encoding='utf-8',\n",
    "            decode_error='ignore',\n",
    "            strip_accents='unicode',\n",
    "            lowercase=True,\n",
    "            preprocessor=None,\n",
    "            tokenizer=None,\n",
    "            stop_words='english',\n",
    "            ngram_range=(1,4),\n",
    "            analyzer='word',\n",
    "            max_df=0.95,\n",
    "            min_df=0.05,\n",
    "            max_features=None\n",
    "                                      )),\n",
    "        ('topicModel', LatentDirichletAllocation())\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.12 s, sys: 84 ms, total: 3.2 s\n",
      "Wall time: 1.78 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('vectorizer',\n",
       "                 CountVectorizer(decode_error='ignore', max_df=0.95,\n",
       "                                 min_df=0.05, ngram_range=(1, 4),\n",
       "                                 stop_words='english',\n",
       "                                 strip_accents='unicode')),\n",
       "                ('topicModel', LatentDirichletAllocation())])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pipe.fit(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 444 ms, sys: 16 ms, total: 460 ms\n",
      "Wall time: 354 ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[5.58786998e-04, 5.58835278e-04, 5.58810665e-04, 5.58830986e-04,\n",
       "        5.58726236e-04, 5.58788708e-04, 5.58755915e-04, 9.94970880e-01,\n",
       "        5.58764409e-04, 5.58821033e-04],\n",
       "       [5.52590848e-04, 5.52621474e-04, 5.52611820e-04, 3.16600192e-01,\n",
       "        6.78978861e-01, 5.52645043e-04, 5.52587242e-04, 5.52646139e-04,\n",
       "        5.52580132e-04, 5.52664092e-04],\n",
       "       [6.62382118e-04, 6.62394030e-04, 2.90947508e-01, 4.22067356e-02,\n",
       "        6.62209013e-01, 6.62442065e-04, 6.62331985e-04, 6.62430880e-04,\n",
       "        6.62390448e-04, 6.62371573e-04],\n",
       "       [4.48534654e-04, 4.48641123e-04, 9.72985970e-01, 4.48566315e-04,\n",
       "        2.34256814e-02, 4.48566196e-04, 4.48471356e-04, 4.48555830e-04,\n",
       "        4.48491308e-04, 4.48521557e-04],\n",
       "       [4.29299149e-04, 4.29307709e-04, 4.74595751e-01, 3.04425475e-02,\n",
       "        6.17195727e-02, 4.29288742e-04, 4.29274461e-04, 4.30666347e-01,\n",
       "        4.29316808e-04, 4.29294370e-04],\n",
       "       [1.58751478e-03, 1.58773773e-03, 2.87715349e-01, 1.58761542e-03,\n",
       "        1.58759376e-03, 1.58757430e-03, 1.58739870e-03, 6.99584052e-01,\n",
       "        1.58764369e-03, 1.58752009e-03],\n",
       "       [1.85222097e-04, 1.85242188e-04, 9.98332955e-01, 1.85223724e-04,\n",
       "        1.85217369e-04, 1.85230287e-04, 1.85222300e-04, 1.85234952e-04,\n",
       "        1.85228955e-04, 1.85222998e-04],\n",
       "       [2.77639724e-02, 1.38451281e-01, 4.69613891e-04, 4.69613417e-04,\n",
       "        4.69541416e-04, 8.30497584e-01, 4.69595101e-04, 4.69634112e-04,\n",
       "        4.69579660e-04, 4.69585089e-04],\n",
       "       [3.33397418e-03, 3.33380503e-03, 3.33381198e-03, 3.33456453e-03,\n",
       "        3.33407482e-03, 3.33380281e-03, 9.69994225e-01, 3.33385011e-03,\n",
       "        3.33385098e-03, 3.33404029e-03],\n",
       "       [2.12792131e-03, 4.37378428e-01, 2.12800003e-03, 2.12784036e-03,\n",
       "        2.12788810e-03, 2.12835590e-03, 2.12854265e-03, 4.79782547e-01,\n",
       "        6.79426146e-02, 2.12786178e-03],\n",
       "       [2.76282420e-04, 2.76300913e-04, 2.76312039e-04, 2.76290888e-04,\n",
       "        2.76317791e-04, 9.97513353e-01, 2.76271640e-04, 2.76306365e-04,\n",
       "        2.76287803e-04, 2.76277236e-04],\n",
       "       [2.21774658e-04, 2.21774343e-04, 6.96994604e-01, 2.21774430e-04,\n",
       "        1.05631496e-01, 2.21768567e-04, 2.21793393e-04, 1.95821455e-01,\n",
       "        2.21794515e-04, 2.21764965e-04],\n",
       "       [1.13654624e-03, 4.99689544e-01, 1.16173941e-01, 1.13657678e-03,\n",
       "        1.13660616e-03, 1.13669829e-03, 1.13676759e-03, 3.76180084e-01,\n",
       "        1.13669659e-03, 1.13653977e-03],\n",
       "       [5.68333539e-04, 5.71284592e-01, 1.60771075e-01, 5.68279158e-04,\n",
       "        5.68274706e-04, 6.39023848e-02, 5.68311712e-04, 2.00632156e-01,\n",
       "        5.68334953e-04, 5.68258669e-04],\n",
       "       [1.20492761e-03, 5.30447415e-01, 1.20512699e-03, 1.20512115e-03,\n",
       "        1.20506526e-03, 1.20501663e-03, 1.20514391e-03, 1.20520864e-03,\n",
       "        4.59911962e-01, 1.20501237e-03],\n",
       "       [2.20802214e-04, 2.20813749e-04, 8.30570173e-01, 2.20808407e-04,\n",
       "        8.47693478e-02, 8.31148602e-02, 2.20781777e-04, 2.20813397e-04,\n",
       "        2.20808649e-04, 2.20790358e-04],\n",
       "       [7.46436406e-04, 8.16713132e-01, 7.46452577e-04, 7.46356080e-04,\n",
       "        7.46362224e-04, 5.75318988e-02, 7.46379045e-04, 8.37067630e-02,\n",
       "        3.75696437e-02, 7.46576508e-04],\n",
       "       [2.96779240e-04, 2.96823034e-04, 2.96809630e-04, 2.96787830e-04,\n",
       "        2.96774409e-04, 9.97328861e-01, 2.96788816e-04, 2.96811532e-04,\n",
       "        2.96782194e-04, 2.96782380e-04],\n",
       "       [1.16566893e-01, 2.28313866e-01, 3.04035026e-04, 3.04007497e-04,\n",
       "        3.04034688e-04, 2.79817880e-01, 3.04026034e-04, 3.73477247e-01,\n",
       "        3.04017316e-04, 3.03992340e-04],\n",
       "       [6.33068630e-04, 5.92840650e-01, 2.57350026e-01, 6.33084768e-04,\n",
       "        6.33011298e-04, 1.45378114e-01, 6.33010959e-04, 6.33024985e-04,\n",
       "        6.33016976e-04, 6.32992619e-04],\n",
       "       [3.25810591e-04, 4.94181929e-01, 1.81069924e-02, 3.25814760e-04,\n",
       "        6.98655029e-02, 1.00343254e-01, 3.25821690e-04, 3.15873292e-01,\n",
       "        3.25804638e-04, 3.25778784e-04],\n",
       "       [6.89870543e-04, 6.89808185e-04, 6.89818168e-04, 9.93791769e-01,\n",
       "        6.89764682e-04, 6.89865806e-04, 6.89772652e-04, 6.89771592e-04,\n",
       "        6.89759168e-04, 6.89799763e-04],\n",
       "       [2.55785393e-04, 5.77028282e-01, 1.89452513e-01, 1.62455494e-02,\n",
       "        2.55788724e-04, 2.55826648e-04, 2.55790537e-04, 2.15738825e-01,\n",
       "        2.55840529e-04, 2.55798454e-04],\n",
       "       [2.53197565e-04, 3.64039589e-01, 1.81014065e-01, 1.30985640e-01,\n",
       "        2.53205897e-04, 2.53209175e-04, 2.53191940e-04, 3.22441454e-01,\n",
       "        2.53203834e-04, 2.53244185e-04],\n",
       "       [4.54600757e-03, 4.54607382e-03, 4.54584476e-03, 4.99869818e-02,\n",
       "        4.54555513e-03, 4.54586273e-03, 4.54581999e-03, 4.54596689e-03,\n",
       "        9.13646035e-01, 4.54585250e-03],\n",
       "       [5.55568951e-03, 5.55627561e-03, 5.55590311e-03, 5.55555562e-03,\n",
       "        5.55562069e-03, 5.55606124e-03, 5.55614587e-03, 5.55597278e-03,\n",
       "        9.49997150e-01, 5.55562590e-03],\n",
       "       [2.24255304e-04, 2.24285316e-04, 8.68874364e-01, 2.24282749e-04,\n",
       "        2.24252032e-04, 2.24257850e-04, 2.24240852e-04, 1.29331520e-01,\n",
       "        2.24276088e-04, 2.24266046e-04],\n",
       "       [5.52540746e-04, 5.90778779e-01, 5.52668301e-04, 4.04800393e-01,\n",
       "        5.52592714e-04, 5.52640960e-04, 5.52542661e-04, 5.52645563e-04,\n",
       "        5.52628228e-04, 5.52569019e-04],\n",
       "       [6.75782803e-04, 9.93917692e-01, 6.75821772e-04, 6.75950156e-04,\n",
       "        6.75766654e-04, 6.75790227e-04, 6.75792909e-04, 6.75796569e-04,\n",
       "        6.75793312e-04, 6.75813304e-04],\n",
       "       [3.95324670e-04, 9.96442134e-01, 3.95352685e-04, 3.95316405e-04,\n",
       "        3.95301846e-04, 3.95325713e-04, 3.95309067e-04, 3.95332103e-04,\n",
       "        3.95317945e-04, 3.95285414e-04],\n",
       "       [1.66689251e-03, 5.21710122e-01, 1.19570683e-01, 2.17110367e-01,\n",
       "        1.66709896e-03, 1.66701838e-03, 1.66683170e-03, 1.31607048e-01,\n",
       "        1.66699500e-03, 1.66694309e-03],\n",
       "       [5.65127877e-04, 3.35969769e-01, 2.70582248e-01, 5.65164374e-04,\n",
       "        5.65085803e-04, 5.65166077e-04, 5.65031693e-04, 3.89492246e-01,\n",
       "        5.65119920e-04, 5.65041172e-04],\n",
       "       [4.42549236e-04, 4.42602636e-04, 5.58048613e-01, 3.40278263e-01,\n",
       "        4.42548124e-04, 4.42545838e-04, 4.42522536e-04, 4.42593095e-04,\n",
       "        9.85752048e-02, 4.42557167e-04],\n",
       "       [5.23645320e-04, 5.23627208e-04, 5.23640529e-04, 5.23712423e-04,\n",
       "        1.57304990e-01, 5.23654989e-04, 5.23658270e-04, 8.38505819e-01,\n",
       "        5.23630383e-04, 5.23622000e-04],\n",
       "       [4.00125172e-04, 9.96399094e-01, 4.00114415e-04, 4.00141184e-04,\n",
       "        4.00108311e-04, 4.00080345e-04, 4.00060469e-04, 4.00120408e-04,\n",
       "        4.00082898e-04, 4.00072895e-04],\n",
       "       [3.59789308e-04, 3.59784335e-04, 3.59800620e-04, 5.46861174e-01,\n",
       "        3.26493876e-01, 3.59764419e-04, 3.59788029e-04, 3.59803752e-04,\n",
       "        1.24126449e-01, 3.59769993e-04],\n",
       "       [6.17410745e-04, 6.17418055e-04, 6.17403383e-04, 6.17374211e-04,\n",
       "        6.17459263e-04, 6.17364033e-04, 9.94443438e-01, 6.17405377e-04,\n",
       "        6.17377874e-04, 6.17349375e-04],\n",
       "       [4.23798057e-04, 4.23831164e-04, 5.37756392e-02, 5.56880639e-01,\n",
       "        4.23768331e-04, 4.23788079e-04, 4.23766959e-04, 3.86377122e-01,\n",
       "        4.23799311e-04, 4.23847854e-04],\n",
       "       [2.61145109e-04, 2.61136824e-04, 9.97649836e-01, 2.61124476e-04,\n",
       "        2.61130533e-04, 2.61118637e-04, 2.61121374e-04, 2.61134832e-04,\n",
       "        2.61132192e-04, 2.61119838e-04],\n",
       "       [9.97336981e-01, 2.95903183e-04, 2.95889135e-04, 2.95886154e-04,\n",
       "        2.95902413e-04, 2.95884955e-04, 2.95888183e-04, 2.95891779e-04,\n",
       "        2.95893134e-04, 2.95879607e-04],\n",
       "       [2.45741773e-04, 2.45771621e-04, 9.97788171e-01, 2.45759863e-04,\n",
       "        2.45763034e-04, 2.45756182e-04, 2.45740440e-04, 2.45780386e-04,\n",
       "        2.45763865e-04, 2.45752037e-04],\n",
       "       [3.63740829e-04, 1.65798413e-01, 2.80276159e-01, 3.63709997e-04,\n",
       "        1.73955367e-01, 2.22785898e-02, 3.07701971e-02, 3.25466389e-01,\n",
       "        3.63714690e-04, 3.63718795e-04],\n",
       "       [2.66001258e-04, 2.66011670e-04, 2.66026599e-04, 2.66029668e-04,\n",
       "        2.66015933e-04, 2.65994287e-04, 2.65982580e-04, 5.72176269e-01,\n",
       "        2.65996054e-04, 4.25695673e-01],\n",
       "       [2.25262461e-04, 2.25258799e-04, 9.97972722e-01, 2.25251921e-04,\n",
       "        2.25251335e-04, 2.25245489e-04, 2.25244731e-04, 2.25259820e-04,\n",
       "        2.25254493e-04, 2.25248925e-04],\n",
       "       [1.74243960e-04, 1.74265746e-04, 5.09444900e-01, 1.74276695e-04,\n",
       "        1.74240941e-04, 1.74246272e-04, 1.74231651e-04, 4.89161097e-01,\n",
       "        1.74254700e-04, 1.74243059e-04],\n",
       "       [9.97375787e-01, 2.91589666e-04, 2.91577608e-04, 2.91575706e-04,\n",
       "        2.91588320e-04, 2.91572493e-04, 2.91574508e-04, 2.91583323e-04,\n",
       "        2.91580311e-04, 2.91570752e-04],\n",
       "       [4.53307003e-02, 4.95593215e-02, 3.51475223e-01, 6.66845447e-04,\n",
       "        8.04239751e-02, 4.69876695e-01, 6.66752605e-04, 6.66872617e-04,\n",
       "        6.66817483e-04, 6.66797036e-04],\n",
       "       [3.16524498e-04, 3.16517967e-04, 3.16523014e-04, 3.16520822e-04,\n",
       "        3.16631873e-04, 3.16504709e-04, 3.16516447e-04, 9.97151214e-01,\n",
       "        3.16535468e-04, 3.16511370e-04],\n",
       "       [9.94914005e-01, 5.65101662e-04, 5.65112711e-04, 5.65084305e-04,\n",
       "        5.65136747e-04, 5.65075751e-04, 5.65204276e-04, 5.65107957e-04,\n",
       "        5.65089746e-04, 5.65081652e-04],\n",
       "       [2.84989033e-04, 1.41500157e-01, 2.31545516e-01, 2.84964291e-04,\n",
       "        2.84993134e-04, 2.85003392e-04, 2.84965773e-04, 6.24959478e-01,\n",
       "        2.84951798e-04, 2.84981547e-04],\n",
       "       [3.93770843e-04, 5.13501806e-01, 4.83348023e-01, 3.93762921e-04,\n",
       "        3.93757876e-04, 3.93818106e-04, 3.93791693e-04, 3.93795636e-04,\n",
       "        3.93743285e-04, 3.93730644e-04],\n",
       "       [2.97651383e-04, 2.97700530e-04, 2.97713871e-04, 2.97704105e-04,\n",
       "        2.97653904e-04, 2.97658115e-04, 2.97647810e-04, 9.97320905e-01,\n",
       "        2.97702388e-04, 2.97663358e-04],\n",
       "       [3.12556337e-04, 3.12582185e-04, 4.68113461e-01, 3.12568325e-04,\n",
       "        1.69016382e-01, 3.12583942e-04, 3.12544497e-04, 3.60682183e-01,\n",
       "        3.12572278e-04, 3.12565470e-04],\n",
       "       [2.39298921e-04, 5.46855704e-01, 2.39295753e-04, 2.39284273e-04,\n",
       "        1.52189232e-01, 2.39295809e-04, 2.39289522e-04, 2.99280036e-01,\n",
       "        2.39298862e-04, 2.39264543e-04],\n",
       "       [1.01024166e-03, 6.85176714e-01, 1.01034818e-03, 1.01033117e-03,\n",
       "        1.01015971e-03, 1.01025830e-03, 1.01027989e-03, 3.06741230e-01,\n",
       "        1.01023399e-03, 1.01020302e-03],\n",
       "       [3.23732172e-04, 4.84035652e-01, 3.23729871e-04, 3.23685891e-04,\n",
       "        3.23733720e-04, 3.23709099e-04, 3.58620620e-01, 1.55077732e-01,\n",
       "        3.23723775e-04, 3.23681390e-04],\n",
       "       [5.21997660e-02, 2.03714588e-04, 6.44112316e-02, 1.87471951e-01,\n",
       "        8.79673535e-02, 2.03709364e-04, 2.03694469e-04, 5.68769927e-01,\n",
       "        3.83649227e-02, 2.03730350e-04],\n",
       "       [5.23674773e-04, 5.81655914e-01, 1.20167547e-01, 5.23715857e-04,\n",
       "        5.23631420e-04, 5.23630362e-04, 5.23614985e-04, 2.94510900e-01,\n",
       "        5.23661803e-04, 5.23709859e-04],\n",
       "       [5.37743071e-04, 5.37806939e-04, 7.51358294e-01, 2.44339620e-01,\n",
       "        5.37728424e-04, 5.37746084e-04, 5.37725204e-04, 5.37792505e-04,\n",
       "        5.37755243e-04, 5.37789008e-04],\n",
       "       [1.71958508e-01, 1.90531187e-04, 3.63901620e-01, 1.90512145e-04,\n",
       "        2.00714092e-01, 1.90518399e-04, 1.90518652e-04, 2.62282662e-01,\n",
       "        1.90521616e-04, 1.90515386e-04],\n",
       "       [8.30346321e-02, 2.01828917e-01, 3.74635039e-04, 3.74639588e-04,\n",
       "        2.08097786e-01, 3.74639937e-04, 3.74580708e-04, 4.36667144e-01,\n",
       "        6.84983837e-02, 3.74641978e-04],\n",
       "       [1.40875768e-03, 1.40876950e-03, 1.40883882e-03, 1.40898800e-03,\n",
       "        1.40877205e-03, 9.87320628e-01, 1.40852591e-03, 1.40881846e-03,\n",
       "        1.40900942e-03, 1.40889222e-03],\n",
       "       [1.29309442e-01, 2.41793852e-01, 3.23832621e-01, 2.66718283e-04,\n",
       "        3.03463748e-01, 2.66721783e-04, 2.66704522e-04, 2.66744878e-04,\n",
       "        2.66738325e-04, 2.66708642e-04],\n",
       "       [1.40955942e-01, 2.48825752e-04, 2.88955781e-01, 2.48813408e-04,\n",
       "        1.36196861e-01, 5.68824241e-02, 2.48821680e-04, 3.75764930e-01,\n",
       "        2.48811919e-04, 2.48789295e-04],\n",
       "       [1.80995029e-01, 1.97686761e-04, 7.96779763e-02, 1.97686154e-04,\n",
       "        2.53410849e-01, 1.97677897e-04, 1.97667012e-04, 4.84730076e-01,\n",
       "        1.97684562e-04, 1.97666930e-04],\n",
       "       [4.78551634e-04, 9.95692929e-01, 4.78610862e-04, 4.78543306e-04,\n",
       "        4.78591605e-04, 4.78564916e-04, 4.78511807e-04, 4.78592543e-04,\n",
       "        4.78545406e-04, 4.78558779e-04],\n",
       "       [8.69656339e-04, 9.92172836e-01, 8.69694265e-04, 8.69747933e-04,\n",
       "        8.69629880e-04, 8.69635610e-04, 8.69670461e-04, 8.69723219e-04,\n",
       "        8.69754362e-04, 8.69652003e-04],\n",
       "       [1.97654446e-04, 1.97678978e-04, 6.55481828e-02, 1.97655171e-04,\n",
       "        1.97678727e-04, 1.97702010e-04, 1.97660172e-04, 9.32870476e-01,\n",
       "        1.97662496e-04, 1.97649245e-04],\n",
       "       [4.78607980e-04, 4.78595011e-04, 4.78577520e-04, 4.78538695e-04,\n",
       "        3.05607121e-01, 4.78550514e-04, 4.78596365e-04, 6.90564226e-01,\n",
       "        4.78632186e-04, 4.78555051e-04],\n",
       "       [2.53890318e-04, 2.53874940e-04, 2.73822935e-01, 2.53860816e-04,\n",
       "        3.37336512e-01, 2.53855575e-04, 2.53856927e-04, 3.87063482e-01,\n",
       "        2.53878147e-04, 2.53854342e-04],\n",
       "       [4.44473580e-04, 4.44499348e-04, 4.44517829e-04, 9.95999584e-01,\n",
       "        4.44492651e-04, 4.44486140e-04, 4.44469233e-04, 4.44509476e-04,\n",
       "        4.44474951e-04, 4.44493262e-04],\n",
       "       [1.43318664e-01, 3.78912017e-04, 1.98037556e-01, 3.78914416e-04,\n",
       "        3.88232244e-01, 3.78852729e-04, 3.78848827e-04, 1.24756901e-01,\n",
       "        1.43760230e-01, 3.78875882e-04],\n",
       "       [6.45222927e-04, 6.45386800e-04, 1.80369940e-01, 8.14467775e-01,\n",
       "        6.45279710e-04, 6.45332275e-04, 6.45270194e-04, 6.45323767e-04,\n",
       "        6.45192706e-04, 6.45276414e-04],\n",
       "       [3.78844213e-04, 6.96336042e-02, 3.78891646e-04, 3.78880133e-04,\n",
       "        3.08797047e-02, 3.78888952e-04, 1.35359221e-01, 7.61854223e-01,\n",
       "        3.78841934e-04, 3.78899882e-04],\n",
       "       [2.93313975e-04, 2.93343430e-04, 2.93345154e-04, 2.93322170e-04,\n",
       "        6.31820623e-01, 2.93337289e-04, 2.93309553e-04, 3.65832784e-01,\n",
       "        2.93325415e-04, 2.93295755e-04],\n",
       "       [2.93337069e-04, 9.97360150e-01, 2.93325672e-04, 2.93317830e-04,\n",
       "        2.93295082e-04, 2.93318090e-04, 2.93320050e-04, 2.93328984e-04,\n",
       "        2.93301924e-04, 2.93305748e-04],\n",
       "       [1.65603896e-04, 1.65611932e-04, 1.65601466e-04, 1.65589232e-04,\n",
       "        1.65604166e-04, 1.65604511e-04, 1.65605191e-04, 9.98509613e-01,\n",
       "        1.65586539e-04, 1.65579981e-04],\n",
       "       [2.15120767e-04, 2.15116440e-04, 8.36436614e-01, 2.15126648e-04,\n",
       "        1.61842509e-01, 2.15105552e-04, 2.15093968e-04, 2.15117928e-04,\n",
       "        2.15104248e-04, 2.15091392e-04],\n",
       "       [5.95346252e-04, 8.57541082e-02, 5.82069044e-01, 3.28009237e-01,\n",
       "        5.95350097e-04, 5.95322039e-04, 5.95369721e-04, 5.95423277e-04,\n",
       "        5.95397271e-04, 5.95403050e-04],\n",
       "       [3.37916892e-04, 3.37906378e-04, 3.37915996e-04, 7.62675953e-01,\n",
       "        2.34620766e-01, 3.37899402e-04, 3.37871031e-04, 3.37958148e-04,\n",
       "        3.37908746e-04, 3.37903832e-04],\n",
       "       [3.53455333e-04, 3.53457732e-04, 6.93944370e-01, 3.53419953e-04,\n",
       "        3.53484220e-04, 3.03228138e-01, 3.53416056e-04, 3.53454955e-04,\n",
       "        3.53417527e-04, 3.53386103e-04],\n",
       "       [8.92960576e-04, 5.93350561e-01, 8.93081483e-04, 1.52604060e-01,\n",
       "        8.93094288e-04, 8.93053590e-04, 8.93009881e-04, 2.47794166e-01,\n",
       "        8.93027799e-04, 8.92986039e-04],\n",
       "       [2.09697834e-04, 2.09709543e-04, 2.88457313e-01, 2.09675261e-04,\n",
       "        2.09705106e-04, 2.09696426e-04, 2.09680357e-04, 7.09865154e-01,\n",
       "        2.09701711e-04, 2.09666411e-04],\n",
       "       [3.90690517e-04, 6.40418839e-01, 2.95847763e-01, 3.90742659e-04,\n",
       "        3.90671949e-04, 3.90699022e-04, 3.90667548e-04, 3.90725097e-04,\n",
       "        6.09984665e-02, 3.90735321e-04],\n",
       "       [4.69624728e-04, 8.16889275e-01, 1.13356457e-01, 6.64672853e-02,\n",
       "        4.69528398e-04, 4.69531776e-04, 4.69559707e-04, 4.69592866e-04,\n",
       "        4.69585082e-04, 4.69560070e-04],\n",
       "       [4.42591078e-04, 6.14708089e-01, 4.42622523e-04, 4.42582906e-04,\n",
       "        4.42604892e-04, 4.42583308e-04, 4.42593912e-04, 3.04386389e-01,\n",
       "        7.78073916e-02, 4.42552236e-04],\n",
       "       [2.32077439e-04, 2.32099852e-04, 9.97911348e-01, 2.32058821e-04,\n",
       "        2.32071474e-04, 2.32083914e-04, 2.32069450e-04, 2.32082549e-04,\n",
       "        2.32070085e-04, 2.32038169e-04],\n",
       "       [4.01676595e-04, 9.96384981e-01, 4.01697618e-04, 4.01672705e-04,\n",
       "        4.01644350e-04, 4.01666124e-04, 4.01642752e-04, 4.01678886e-04,\n",
       "        4.01665415e-04, 4.01674069e-04],\n",
       "       [5.18195758e-04, 5.18175684e-04, 5.18185526e-04, 9.95336287e-01,\n",
       "        5.18182138e-04, 5.18184107e-04, 5.18167819e-04, 5.18197065e-04,\n",
       "        5.18168772e-04, 5.18255940e-04],\n",
       "       [3.47270338e-04, 3.47273315e-04, 3.47272671e-04, 3.47297472e-04,\n",
       "        3.47251613e-04, 3.47257499e-04, 3.47239856e-04, 3.47273528e-04,\n",
       "        3.47266904e-04, 9.96874597e-01],\n",
       "       [2.14169078e-04, 2.14166866e-04, 9.98072561e-01, 2.14158060e-04,\n",
       "        2.14157944e-04, 2.14151952e-04, 2.14151024e-04, 2.14168542e-04,\n",
       "        2.14160357e-04, 2.14155292e-04],\n",
       "       [3.41359447e-04, 8.92748579e-03, 3.91450656e-01, 3.41390116e-04,\n",
       "        3.41391504e-04, 4.86374889e-01, 3.41382692e-04, 1.11198760e-01,\n",
       "        3.41343593e-04, 3.41342371e-04],\n",
       "       [4.52552039e-04, 4.52615410e-04, 4.52551651e-04, 4.52532817e-04,\n",
       "        4.52527882e-04, 4.52562397e-04, 9.95927061e-01, 4.52551128e-04,\n",
       "        4.52532818e-04, 4.52513300e-04],\n",
       "       [1.03110879e-03, 6.04338134e-01, 1.74832147e-01, 5.08356569e-02,\n",
       "        1.03114126e-03, 1.03107220e-03, 1.03109747e-03, 1.63807445e-01,\n",
       "        1.03111063e-03, 1.03108697e-03],\n",
       "       [3.05850555e-04, 3.05840834e-04, 3.05842772e-04, 3.05832064e-04,\n",
       "        3.05854145e-04, 3.05830914e-04, 3.05831406e-04, 3.05840597e-04,\n",
       "        9.97247452e-01, 3.05824768e-04],\n",
       "       [2.50644028e-04, 2.50650927e-04, 2.50647699e-04, 2.50636152e-04,\n",
       "        9.97744206e-01, 2.50642636e-04, 2.50647872e-04, 2.50646615e-04,\n",
       "        2.50643415e-04, 2.50634605e-04],\n",
       "       [3.05850555e-04, 3.05840834e-04, 3.05842772e-04, 3.05832064e-04,\n",
       "        3.05854145e-04, 3.05830914e-04, 3.05831406e-04, 3.05840597e-04,\n",
       "        9.97247452e-01, 3.05824768e-04],\n",
       "       [2.50644028e-04, 2.50650927e-04, 2.50647699e-04, 2.50636152e-04,\n",
       "        9.97744206e-01, 2.50642636e-04, 2.50647872e-04, 2.50646615e-04,\n",
       "        2.50643415e-04, 2.50634605e-04],\n",
       "       [3.05850555e-04, 3.05840834e-04, 3.05842772e-04, 3.05832064e-04,\n",
       "        3.05854145e-04, 3.05830914e-04, 3.05831406e-04, 3.05840597e-04,\n",
       "        9.97247452e-01, 3.05824768e-04],\n",
       "       [2.50644028e-04, 2.50650927e-04, 2.50647699e-04, 2.50636152e-04,\n",
       "        9.97744206e-01, 2.50642636e-04, 2.50647872e-04, 2.50646615e-04,\n",
       "        2.50643415e-04, 2.50634605e-04]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%time pipe.transform(descriptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['000',\n",
       " '10',\n",
       " '10 years',\n",
       " '100',\n",
       " '12',\n",
       " '15',\n",
       " '2020',\n",
       " '40',\n",
       " '401',\n",
       " 'aa',\n",
       " 'abilities',\n",
       " 'ability',\n",
       " 'ability communicate',\n",
       " 'ability manage',\n",
       " 'ability work',\n",
       " 'ability work effectively',\n",
       " 'ability work independently',\n",
       " 'able',\n",
       " 'accept',\n",
       " 'access',\n",
       " 'accommodation',\n",
       " 'accommodations',\n",
       " 'according',\n",
       " 'account',\n",
       " 'accountability',\n",
       " 'accuracy',\n",
       " 'accurate',\n",
       " 'achieve',\n",
       " 'acquisition',\n",
       " 'act',\n",
       " 'action',\n",
       " 'action employer',\n",
       " 'action employer qualified',\n",
       " 'action employer qualified applicants',\n",
       " 'actionable',\n",
       " 'active',\n",
       " 'actively',\n",
       " 'activities',\n",
       " 'acumen',\n",
       " 'ad',\n",
       " 'addition',\n",
       " 'additional',\n",
       " 'address',\n",
       " 'advanced',\n",
       " 'advanced degree',\n",
       " 'advancing',\n",
       " 'affirmative',\n",
       " 'affirmative action',\n",
       " 'affirmative action employer',\n",
       " 'affirmative action employer qualified',\n",
       " 'age',\n",
       " 'age marital',\n",
       " 'age marital status',\n",
       " 'age national',\n",
       " 'age national origin',\n",
       " 'agencies',\n",
       " 'agency',\n",
       " 'agile',\n",
       " 'ai',\n",
       " 'air',\n",
       " 'airflow',\n",
       " 'algorithms',\n",
       " 'alignment',\n",
       " 'allow',\n",
       " 'analyses',\n",
       " 'analysis',\n",
       " 'analyst',\n",
       " 'analytic',\n",
       " 'analytical',\n",
       " 'analytical skills',\n",
       " 'analytics',\n",
       " 'analytics data',\n",
       " 'analytics team',\n",
       " 'analytics tools',\n",
       " 'analyze',\n",
       " 'analyzing',\n",
       " 'ancestry',\n",
       " 'annual',\n",
       " 'answer',\n",
       " 'apache',\n",
       " 'apache spark',\n",
       " 'api',\n",
       " 'apis',\n",
       " 'app',\n",
       " 'applicable',\n",
       " 'applicant',\n",
       " 'applicants',\n",
       " 'applicants receive',\n",
       " 'applicants receive consideration',\n",
       " 'applicants receive consideration employment',\n",
       " 'application',\n",
       " 'application development',\n",
       " 'applications',\n",
       " 'apply',\n",
       " 'applying',\n",
       " 'approach',\n",
       " 'approaches',\n",
       " 'appropriate',\n",
       " 'aptitude',\n",
       " 'architect',\n",
       " 'architects',\n",
       " 'architecture',\n",
       " 'area',\n",
       " 'areas',\n",
       " 'artificial',\n",
       " 'artificial intelligence',\n",
       " 'ask',\n",
       " 'aspects',\n",
       " 'assessment',\n",
       " 'asset',\n",
       " 'assigned',\n",
       " 'assist',\n",
       " 'assistance',\n",
       " 'assists',\n",
       " 'associate',\n",
       " 'associated',\n",
       " 'associates',\n",
       " 'assurance',\n",
       " 'attention',\n",
       " 'attitude',\n",
       " 'audiences',\n",
       " 'automate',\n",
       " 'automation',\n",
       " 'availability',\n",
       " 'available',\n",
       " 'aws',\n",
       " 'azure',\n",
       " 'bachelor',\n",
       " 'bachelor degree',\n",
       " 'bachelors',\n",
       " 'backed',\n",
       " 'background',\n",
       " 'backgrounds',\n",
       " 'base',\n",
       " 'based',\n",
       " 'based business',\n",
       " 'basic',\n",
       " 'basic qualifications',\n",
       " 'basis',\n",
       " 'basis race',\n",
       " 'basis race religion',\n",
       " 'basis race religion color',\n",
       " 'batch',\n",
       " 'beginning',\n",
       " 'believe',\n",
       " 'belonging',\n",
       " 'benefits',\n",
       " 'benefits package',\n",
       " 'best',\n",
       " 'best practices',\n",
       " 'better',\n",
       " 'better committed',\n",
       " 'bi',\n",
       " 'big',\n",
       " 'big data',\n",
       " 'brands',\n",
       " 'bring',\n",
       " 'broad',\n",
       " 'bs',\n",
       " 'bs degree',\n",
       " 'build',\n",
       " 'build maintain',\n",
       " 'building',\n",
       " 'built',\n",
       " 'business',\n",
       " 'business decisions',\n",
       " 'business intelligence',\n",
       " 'business needs',\n",
       " 'business partners',\n",
       " 'business problems',\n",
       " 'business requirements',\n",
       " 'business stakeholders',\n",
       " 'business teams',\n",
       " 'businesses',\n",
       " 'calls',\n",
       " 'candidate',\n",
       " 'candidates',\n",
       " 'capabilities',\n",
       " 'capability',\n",
       " 'capable',\n",
       " 'capital',\n",
       " 'care',\n",
       " 'career',\n",
       " 'case',\n",
       " 'category',\n",
       " 'cd',\n",
       " 'center',\n",
       " 'centers',\n",
       " 'certifications',\n",
       " 'challenge',\n",
       " 'challenges',\n",
       " 'challenging',\n",
       " 'chance',\n",
       " 'change',\n",
       " 'changes',\n",
       " 'changing',\n",
       " 'characteristic',\n",
       " 'characteristic protected',\n",
       " 'characteristic protected law',\n",
       " 'characteristics',\n",
       " 'check',\n",
       " 'ci',\n",
       " 'ci cd',\n",
       " 'citizenship',\n",
       " 'citizenship status',\n",
       " 'clarity',\n",
       " 'class',\n",
       " 'classification',\n",
       " 'clearance',\n",
       " 'clearly',\n",
       " 'click',\n",
       " 'client',\n",
       " 'clients',\n",
       " 'clinical',\n",
       " 'closely',\n",
       " 'cloud',\n",
       " 'cloud based',\n",
       " 'cloud platform',\n",
       " 'code',\n",
       " 'coding',\n",
       " 'collaborate',\n",
       " 'collaborating',\n",
       " 'collaboration',\n",
       " 'collaborative',\n",
       " 'collaborative spirit',\n",
       " 'collaborative team',\n",
       " 'collaboratively',\n",
       " 'colleagues',\n",
       " 'collect',\n",
       " 'collection',\n",
       " 'college',\n",
       " 'color',\n",
       " 'color national',\n",
       " 'color national origin',\n",
       " 'color religion',\n",
       " 'color religion sex',\n",
       " 'color religion sex gender',\n",
       " 'color religion sex sexual',\n",
       " 'com',\n",
       " 'combination',\n",
       " 'combination education',\n",
       " 'comfortable',\n",
       " 'commercial',\n",
       " 'commit',\n",
       " 'commitment',\n",
       " 'committed',\n",
       " 'committed fostering',\n",
       " 'common',\n",
       " 'communicate',\n",
       " 'communicating',\n",
       " 'communication',\n",
       " 'communication skills',\n",
       " 'communication skills ability',\n",
       " 'communication skills including',\n",
       " 'communications',\n",
       " 'communities',\n",
       " 'communities serve',\n",
       " 'community',\n",
       " 'companies',\n",
       " 'company',\n",
       " 'company policies',\n",
       " 'compensation',\n",
       " 'competencies',\n",
       " 'competitive',\n",
       " 'competitive salary',\n",
       " 'complete',\n",
       " 'completing',\n",
       " 'complex',\n",
       " 'complex data',\n",
       " 'compliance',\n",
       " 'components',\n",
       " 'comprehensive',\n",
       " 'computer',\n",
       " 'computer science',\n",
       " 'computer science related',\n",
       " 'computer skills',\n",
       " 'concept',\n",
       " 'concepts',\n",
       " 'concurrently',\n",
       " 'condition',\n",
       " 'conditions',\n",
       " 'conduct',\n",
       " 'conducting',\n",
       " 'confidential',\n",
       " 'confidentiality',\n",
       " 'consider',\n",
       " 'consideration',\n",
       " 'consideration employment',\n",
       " 'consideration employment regard',\n",
       " 'consideration employment regard race',\n",
       " 'considered',\n",
       " 'consistent',\n",
       " 'constantly',\n",
       " 'consulting',\n",
       " 'consumer',\n",
       " 'contact',\n",
       " 'content',\n",
       " 'continue',\n",
       " 'continuous',\n",
       " 'continuous improvement',\n",
       " 'continuously',\n",
       " 'contract',\n",
       " 'contractors',\n",
       " 'contribute',\n",
       " 'contributing',\n",
       " 'control',\n",
       " 'controls',\n",
       " 'coordinate',\n",
       " 'core',\n",
       " 'core values',\n",
       " 'corporate',\n",
       " 'cost',\n",
       " 'country',\n",
       " 'coverage',\n",
       " 'create',\n",
       " 'creating',\n",
       " 'creation',\n",
       " 'creative',\n",
       " 'creed',\n",
       " 'criminal',\n",
       " 'criminal histories',\n",
       " 'criteria',\n",
       " 'critical',\n",
       " 'crm',\n",
       " 'cross',\n",
       " 'cross functional',\n",
       " 'culture',\n",
       " 'curiosity',\n",
       " 'curious',\n",
       " 'current',\n",
       " 'currently',\n",
       " 'customer',\n",
       " 'customers',\n",
       " 'cycle',\n",
       " 'daily',\n",
       " 'dashboards',\n",
       " 'data analysis',\n",
       " 'data analyst',\n",
       " 'data analytics',\n",
       " 'data architecture',\n",
       " 'data center',\n",
       " 'data collection',\n",
       " 'data data',\n",
       " 'data driven',\n",
       " 'data engineer',\n",
       " 'data engineering',\n",
       " 'data engineers',\n",
       " 'data entry',\n",
       " 'data governance',\n",
       " 'data integrity',\n",
       " 'data management',\n",
       " 'data mining',\n",
       " 'data modeling',\n",
       " 'data models',\n",
       " 'data pipelines',\n",
       " 'data platform',\n",
       " 'data processing',\n",
       " 'data quality',\n",
       " 'data science',\n",
       " 'data scientist',\n",
       " 'data services',\n",
       " 'data sets',\n",
       " 'data sources',\n",
       " 'data stores',\n",
       " 'data structures',\n",
       " 'data systems',\n",
       " 'data visualization',\n",
       " 'data warehouse',\n",
       " 'data warehousing',\n",
       " 'database',\n",
       " 'databases',\n",
       " 'datasets',\n",
       " 'date',\n",
       " 'day',\n",
       " 'db',\n",
       " 'deadlines',\n",
       " 'decision',\n",
       " 'decision making',\n",
       " 'decisions',\n",
       " 'dedicated',\n",
       " 'deep',\n",
       " 'deep understanding',\n",
       " 'defense',\n",
       " 'define',\n",
       " 'defined',\n",
       " 'defining',\n",
       " 'definition',\n",
       " 'degree',\n",
       " 'degree computer',\n",
       " 'degree computer science',\n",
       " 'degree preferred',\n",
       " 'degree required',\n",
       " 'deliver',\n",
       " 'deliverables',\n",
       " 'delivering',\n",
       " 'delivery',\n",
       " 'demands',\n",
       " 'demonstrate',\n",
       " 'demonstrated',\n",
       " 'demonstrated experience',\n",
       " 'dental',\n",
       " 'dental vision',\n",
       " 'department',\n",
       " 'departments',\n",
       " 'deploying',\n",
       " 'deployment',\n",
       " 'described',\n",
       " 'description',\n",
       " 'design',\n",
       " 'design build',\n",
       " 'design data',\n",
       " 'design development',\n",
       " 'design implement',\n",
       " 'designed',\n",
       " 'designing',\n",
       " 'designs',\n",
       " 'desire',\n",
       " 'desired',\n",
       " 'details',\n",
       " 'determine',\n",
       " 'determined',\n",
       " 'develop',\n",
       " 'develop data',\n",
       " 'develop solutions',\n",
       " 'developers',\n",
       " 'developing',\n",
       " 'development',\n",
       " 'development data',\n",
       " 'development experience',\n",
       " 'development opportunities',\n",
       " 'develops',\n",
       " 'devops',\n",
       " 'different',\n",
       " 'digital',\n",
       " 'digitally',\n",
       " 'dimensional',\n",
       " 'direct',\n",
       " 'direction',\n",
       " 'directly',\n",
       " 'director',\n",
       " 'disabilities',\n",
       " 'disability',\n",
       " 'disability status',\n",
       " 'disability veteran',\n",
       " 'disability veteran status',\n",
       " 'discipline',\n",
       " 'disciplines',\n",
       " 'discrepancies',\n",
       " 'discretion',\n",
       " 'discriminate',\n",
       " 'discriminate basis',\n",
       " 'discriminate basis race',\n",
       " 'distributed',\n",
       " 'diverse',\n",
       " 'diversity',\n",
       " 'diversity inclusion',\n",
       " 'division',\n",
       " 'docker',\n",
       " 'document',\n",
       " 'documentation',\n",
       " 'documents',\n",
       " 'does',\n",
       " 'doing',\n",
       " 'domain',\n",
       " 'domains',\n",
       " 'domestic',\n",
       " 'drive',\n",
       " 'driven',\n",
       " 'drives',\n",
       " 'driving',\n",
       " 'drug',\n",
       " 'duties',\n",
       " 'duties assigned',\n",
       " 'dynamic',\n",
       " 'early',\n",
       " 'easy',\n",
       " 'ecosystem',\n",
       " 'education',\n",
       " 'education experience',\n",
       " 'education training',\n",
       " 'education training experience',\n",
       " 'eeo',\n",
       " 'effective',\n",
       " 'effectively',\n",
       " 'effectiveness',\n",
       " 'efficiencies',\n",
       " 'efficiency',\n",
       " 'efficient',\n",
       " 'efficiently',\n",
       " 'efforts',\n",
       " 'electronic',\n",
       " 'eligible',\n",
       " 'email',\n",
       " 'emphasis',\n",
       " 'employee',\n",
       " 'employees',\n",
       " 'employer',\n",
       " 'employer qualified',\n",
       " 'employer qualified applicants',\n",
       " 'employer qualified applicants receive',\n",
       " 'employment',\n",
       " 'employment opportunity',\n",
       " 'employment opportunity employer',\n",
       " 'employment regard',\n",
       " 'employment regard race',\n",
       " 'employment regard race color',\n",
       " 'empower',\n",
       " 'emr',\n",
       " 'enable',\n",
       " 'enablement',\n",
       " 'enabling',\n",
       " 'end',\n",
       " 'end end',\n",
       " 'engagement',\n",
       " 'engineer',\n",
       " 'engineering',\n",
       " 'engineering experience',\n",
       " 'engineering related',\n",
       " 'engineers',\n",
       " 'enhance',\n",
       " 'enjoy',\n",
       " 'ensure',\n",
       " 'ensures',\n",
       " 'ensuring',\n",
       " 'enterprise',\n",
       " 'enterprise data',\n",
       " 'entrepreneurial',\n",
       " 'entrepreneurial spirit',\n",
       " 'entry',\n",
       " 'environment',\n",
       " 'environments',\n",
       " 'eoe',\n",
       " 'equal',\n",
       " 'equal employment',\n",
       " 'equal employment opportunity',\n",
       " 'equal employment opportunity employer',\n",
       " 'equal opportunity',\n",
       " 'equal opportunity affirmative',\n",
       " 'equal opportunity affirmative action',\n",
       " 'equal opportunity employer',\n",
       " 'equipment',\n",
       " 'equity',\n",
       " 'equivalent',\n",
       " 'equivalent combination',\n",
       " 'equivalent combination education',\n",
       " 'essential',\n",
       " 'essential functions',\n",
       " 'establish',\n",
       " 'established',\n",
       " 'ethical',\n",
       " 'etl',\n",
       " 'etl tools',\n",
       " 'evaluate',\n",
       " 'events',\n",
       " 'evolving',\n",
       " 'examples',\n",
       " 'excel',\n",
       " 'excellence',\n",
       " 'excellent',\n",
       " 'excellent communication',\n",
       " 'excellent written',\n",
       " 'exceptional',\n",
       " 'exciting',\n",
       " 'execute',\n",
       " 'executive',\n",
       " 'exercise',\n",
       " 'exhaustive',\n",
       " 'existing',\n",
       " 'existing data',\n",
       " 'expect',\n",
       " 'expected',\n",
       " 'experience building',\n",
       " 'experience data',\n",
       " 'experience developing',\n",
       " 'experience experience',\n",
       " 'experience experience working',\n",
       " 'experience following',\n",
       " 'experience knowledge',\n",
       " 'experience plus',\n",
       " 'experience python',\n",
       " 'experience relevant',\n",
       " 'experience required',\n",
       " 'experience software',\n",
       " 'experience using',\n",
       " 'experience working',\n",
       " 'experience years',\n",
       " 'experience years experience',\n",
       " 'experienced',\n",
       " 'experiences',\n",
       " 'expert',\n",
       " 'expertise',\n",
       " 'experts',\n",
       " 'explain',\n",
       " 'explore',\n",
       " 'expression',\n",
       " 'extensive',\n",
       " 'external',\n",
       " 'externally',\n",
       " 'extract',\n",
       " 'facilitation',\n",
       " 'facilities',\n",
       " 'facility',\n",
       " 'facing',\n",
       " 'familiar',\n",
       " 'familiarity',\n",
       " 'family',\n",
       " 'fast',\n",
       " 'fast paced',\n",
       " 'fast paced environment',\n",
       " 'faster',\n",
       " 'feature',\n",
       " 'features',\n",
       " 'federal',\n",
       " 'federal state',\n",
       " 'federal state local',\n",
       " 'feedback',\n",
       " 'feel',\n",
       " 'female',\n",
       " 'field',\n",
       " 'field equivalent',\n",
       " 'fields',\n",
       " 'finance',\n",
       " 'financial',\n",
       " 'financial management',\n",
       " 'finding',\n",
       " 'firm',\n",
       " 'fit',\n",
       " 'flexibility',\n",
       " 'flexible',\n",
       " 'flow',\n",
       " 'focus',\n",
       " 'focus data',\n",
       " 'focused',\n",
       " 'follow',\n",
       " 'following',\n",
       " 'form',\n",
       " 'forms',\n",
       " 'forward',\n",
       " 'fostering',\n",
       " 'founded',\n",
       " 'framework',\n",
       " 'frameworks',\n",
       " 'free',\n",
       " 'fun',\n",
       " 'function',\n",
       " 'functional',\n",
       " 'functions',\n",
       " 'fundamental',\n",
       " 'funded',\n",
       " 'future',\n",
       " 'gaining',\n",
       " 'gaps',\n",
       " 'gather',\n",
       " 'gcp',\n",
       " 'gender',\n",
       " 'gender gender',\n",
       " 'gender gender identity',\n",
       " 'gender identity',\n",
       " 'gender identity expression',\n",
       " 'gender identity national',\n",
       " 'gender identity national origin',\n",
       " 'gender identity sexual',\n",
       " 'gender identity sexual orientation',\n",
       " 'gender sexual',\n",
       " 'gender sexual orientation',\n",
       " 'general',\n",
       " 'generation',\n",
       " 'generous',\n",
       " 'genetic',\n",
       " 'genetic information',\n",
       " 'getting',\n",
       " 'global',\n",
       " 'globe',\n",
       " 'goal',\n",
       " 'goals',\n",
       " 'good',\n",
       " 'google',\n",
       " 'google cloud',\n",
       " 'governance',\n",
       " 'government',\n",
       " 'graph',\n",
       " 'great',\n",
       " 'ground',\n",
       " 'group',\n",
       " 'groups',\n",
       " 'grow',\n",
       " 'growing',\n",
       " 'growth',\n",
       " 'guidance',\n",
       " 'guide',\n",
       " 'guidelines',\n",
       " 'hadoop',\n",
       " 'handle',\n",
       " 'handling',\n",
       " 'hands',\n",
       " 'hands experience',\n",
       " 'hard',\n",
       " 'headquartered',\n",
       " 'health',\n",
       " 'healthcare',\n",
       " 'help',\n",
       " 'help build',\n",
       " 'help clients',\n",
       " 'help make',\n",
       " 'helpful',\n",
       " 'helping',\n",
       " 'helps',\n",
       " 'high',\n",
       " 'high level',\n",
       " 'highly',\n",
       " 'hire',\n",
       " 'hiring',\n",
       " 'histories',\n",
       " 'hive',\n",
       " 'home',\n",
       " 'hospital',\n",
       " 'hospitals',\n",
       " 'hours',\n",
       " 'hr',\n",
       " 'huge',\n",
       " 'human',\n",
       " 'human resources',\n",
       " 'id',\n",
       " 'ideal',\n",
       " 'ideal candidate',\n",
       " 'ideally',\n",
       " 'ideas',\n",
       " 'identification',\n",
       " 'identify',\n",
       " 'identify opportunities',\n",
       " 'identity',\n",
       " 'identity expression',\n",
       " 'identity national',\n",
       " 'identity national origin',\n",
       " 'identity sexual',\n",
       " 'identity sexual orientation',\n",
       " 'immediate',\n",
       " 'impact',\n",
       " 'implement',\n",
       " 'implement data',\n",
       " 'implementation',\n",
       " 'implementing',\n",
       " 'important',\n",
       " 'improve',\n",
       " 'improvement',\n",
       " 'include',\n",
       " 'includes',\n",
       " 'including',\n",
       " 'including limited',\n",
       " 'including pregnancy',\n",
       " 'inclusion',\n",
       " 'inclusive',\n",
       " 'increase',\n",
       " 'independently',\n",
       " 'individual',\n",
       " 'individuals',\n",
       " 'individuals disabilities',\n",
       " 'industry',\n",
       " 'industry experience',\n",
       " 'influence',\n",
       " 'inform',\n",
       " 'informatica',\n",
       " 'information',\n",
       " 'information technology',\n",
       " 'informed',\n",
       " 'infrastructure',\n",
       " 'initiative',\n",
       " 'initiatives',\n",
       " 'innovation',\n",
       " 'innovative',\n",
       " 'insight',\n",
       " 'insights',\n",
       " 'instructions',\n",
       " 'insurance',\n",
       " 'insurance 401',\n",
       " 'integrate',\n",
       " 'integrated',\n",
       " 'integration',\n",
       " 'integrity',\n",
       " 'intelligence',\n",
       " 'intelligent',\n",
       " 'interactions',\n",
       " 'interested',\n",
       " 'interfaces',\n",
       " 'interfacing',\n",
       " 'internal',\n",
       " 'internal external',\n",
       " 'interpersonal',\n",
       " 'interpret',\n",
       " 'interpretation',\n",
       " 'intuitive',\n",
       " 'investigation',\n",
       " 'investment',\n",
       " 'issue',\n",
       " 'issues',\n",
       " 'java',\n",
       " 'java python',\n",
       " 'javascript',\n",
       " 'jazz',\n",
       " 'jazzhr',\n",
       " 'jazzhr jazz',\n",
       " 'job',\n",
       " 'job description',\n",
       " 'job functions',\n",
       " 'jobs',\n",
       " 'jobs2web',\n",
       " 'join',\n",
       " 'join team',\n",
       " 'journey',\n",
       " 'just',\n",
       " 'kafka',\n",
       " 'keeping',\n",
       " 'key',\n",
       " 'know',\n",
       " 'knowledge',\n",
       " 'knowledge data',\n",
       " 'knowledge experience',\n",
       " 'knowledge skills',\n",
       " 'known',\n",
       " 'kubernetes',\n",
       " 'laboratory',\n",
       " 'language',\n",
       " 'languages',\n",
       " 'large',\n",
       " 'large data',\n",
       " 'large data sets',\n",
       " 'large scale',\n",
       " 'largest',\n",
       " 'law',\n",
       " 'laws',\n",
       " 'lead',\n",
       " 'leader',\n",
       " 'leaders',\n",
       " 'leadership',\n",
       " 'leading',\n",
       " 'leads',\n",
       " 'learn',\n",
       " 'learning',\n",
       " 'learning techniques',\n",
       " 'leave',\n",
       " 'legal',\n",
       " 'legal requirements',\n",
       " 'let',\n",
       " 'level',\n",
       " 'levels',\n",
       " 'levels management',\n",
       " 'levels organization',\n",
       " 'leverage',\n",
       " 'leverage data',\n",
       " 'liaison',\n",
       " 'license',\n",
       " 'life',\n",
       " 'life cycle',\n",
       " 'life insurance',\n",
       " 'lifecycle',\n",
       " 'like',\n",
       " 'limited',\n",
       " 'line',\n",
       " 'list',\n",
       " 'live',\n",
       " 'lives',\n",
       " 'll',\n",
       " 'local',\n",
       " 'local law',\n",
       " 'located',\n",
       " 'location',\n",
       " 'locations',\n",
       " 'long',\n",
       " 'long term',\n",
       " 'look',\n",
       " 'looking',\n",
       " 'looking data',\n",
       " 'love',\n",
       " 'machine',\n",
       " 'machine learning',\n",
       " 'machine learning techniques',\n",
       " 'maintain',\n",
       " 'maintain data',\n",
       " 'maintaining',\n",
       " 'maintains',\n",
       " 'maintenance',\n",
       " 'major',\n",
       " 'make',\n",
       " 'makes',\n",
       " 'makes better',\n",
       " 'making',\n",
       " 'manage',\n",
       " 'manage multiple',\n",
       " 'manage multiple projects',\n",
       " 'management',\n",
       " 'management data',\n",
       " 'management experience',\n",
       " 'management skills',\n",
       " 'manager',\n",
       " 'managers',\n",
       " 'manages',\n",
       " 'managing',\n",
       " 'manner',\n",
       " 'marital',\n",
       " 'marital status',\n",
       " 'market',\n",
       " 'marketing',\n",
       " 'markets',\n",
       " 'master',\n",
       " 'master degree',\n",
       " 'masters',\n",
       " 'match',\n",
       " 'mathematics',\n",
       " 'matter',\n",
       " 'matters',\n",
       " 'means',\n",
       " 'measure',\n",
       " 'measures',\n",
       " 'mechanical',\n",
       " 'medical',\n",
       " 'medical condition',\n",
       " 'meet',\n",
       " 'meetings',\n",
       " 'member',\n",
       " 'members',\n",
       " 'mental',\n",
       " 'mental disability',\n",
       " 'methodologies',\n",
       " 'methodology',\n",
       " 'methods',\n",
       " 'metrics',\n",
       " 'microsoft',\n",
       " 'microsoft excel',\n",
       " 'microsoft office',\n",
       " 'military',\n",
       " 'millions',\n",
       " 'minimum',\n",
       " 'minimum qualifications',\n",
       " 'minimum years',\n",
       " 'mining',\n",
       " 'minority',\n",
       " 'mission',\n",
       " 'mission critical',\n",
       " 'ml',\n",
       " 'mobile',\n",
       " 'model',\n",
       " 'modeling',\n",
       " 'models',\n",
       " 'modern',\n",
       " 'money',\n",
       " 'monitor',\n",
       " 'monitoring',\n",
       " 'motivated',\n",
       " 'ms',\n",
       " 'ms office',\n",
       " 'multi',\n",
       " 'multiple',\n",
       " 'multiple projects',\n",
       " 'multiple sources',\n",
       " 'national',\n",
       " 'national origin',\n",
       " 'national origin age',\n",
       " 'national origin disability',\n",
       " 'national origin disability veteran',\n",
       " 'national origin gender',\n",
       " 'national origin gender sexual',\n",
       " 'nature',\n",
       " 'near',\n",
       " 'necessary',\n",
       " 'need',\n",
       " 'needed',\n",
       " 'needs',\n",
       " 'network',\n",
       " 'networks',\n",
       " 'new',\n",
       " 'new data',\n",
       " 'new ways',\n",
       " 'new york',\n",
       " 'nice',\n",
       " 'njob',\n",
       " 'non',\n",
       " 'normal',\n",
       " 'nosql',\n",
       " 'note',\n",
       " 'noverview',\n",
       " 'number',\n",
       " 'objectives',\n",
       " 'objects',\n",
       " 'obtain',\n",
       " 'offer',\n",
       " 'offerings',\n",
       " 'offers',\n",
       " 'office',\n",
       " 'offices',\n",
       " 'offices world',\n",
       " 'old',\n",
       " 'ongoing',\n",
       " 'online',\n",
       " 'open',\n",
       " 'open source',\n",
       " 'operate',\n",
       " 'operating',\n",
       " 'operation',\n",
       " 'operational',\n",
       " 'operationalize',\n",
       " ...]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.named_steps['vectorizer'].get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "class CountVectorizer(_VectorizerMixin, BaseEstimator):\n",
       "    \"\"\"Convert a collection of text documents to a matrix of token counts\n",
       "\n",
       "    This implementation produces a sparse representation of the counts using\n",
       "    scipy.sparse.csr_matrix.\n",
       "\n",
       "    If you do not provide an a-priori dictionary and you do not use an analyzer\n",
       "    that does some kind of feature selection then the number of features will\n",
       "    be equal to the vocabulary size found by analyzing the data.\n",
       "\n",
       "    Read more in the :ref:`User Guide <text_feature_extraction>`.\n",
       "\n",
       "    Parameters\n",
       "    ----------\n",
       "    input : string {'filename', 'file', 'content'}, default='content'\n",
       "        If 'filename', the sequence passed as an argument to fit is\n",
       "        expected to be a list of filenames that need reading to fetch\n",
       "        the raw content to analyze.\n",
       "\n",
       "        If 'file', the sequence items must have a 'read' method (file-like\n",
       "        object) that is called to fetch the bytes in memory.\n",
       "\n",
       "        Otherwise the input is expected to be a sequence of items that\n",
       "        can be of type string or byte.\n",
       "\n",
       "    encoding : string, default='utf-8'\n",
       "        If bytes or files are given to analyze, this encoding is used to\n",
       "        decode.\n",
       "\n",
       "    decode_error : {'strict', 'ignore', 'replace'}, default='strict'\n",
       "        Instruction on what to do if a byte sequence is given to analyze that\n",
       "        contains characters not of the given `encoding`. By default, it is\n",
       "        'strict', meaning that a UnicodeDecodeError will be raised. Other\n",
       "        values are 'ignore' and 'replace'.\n",
       "\n",
       "    strip_accents : {'ascii', 'unicode'}, default=None\n",
       "        Remove accents and perform other character normalization\n",
       "        during the preprocessing step.\n",
       "        'ascii' is a fast method that only works on characters that have\n",
       "        an direct ASCII mapping.\n",
       "        'unicode' is a slightly slower method that works on any characters.\n",
       "        None (default) does nothing.\n",
       "\n",
       "        Both 'ascii' and 'unicode' use NFKD normalization from\n",
       "        :func:`unicodedata.normalize`.\n",
       "\n",
       "    lowercase : bool, default=True\n",
       "        Convert all characters to lowercase before tokenizing.\n",
       "\n",
       "    preprocessor : callable, default=None\n",
       "        Override the preprocessing (string transformation) stage while\n",
       "        preserving the tokenizing and n-grams generation steps.\n",
       "        Only applies if ``analyzer is not callable``.\n",
       "\n",
       "    tokenizer : callable, default=None\n",
       "        Override the string tokenization step while preserving the\n",
       "        preprocessing and n-grams generation steps.\n",
       "        Only applies if ``analyzer == 'word'``.\n",
       "\n",
       "    stop_words : string {'english'}, list, default=None\n",
       "        If 'english', a built-in stop word list for English is used.\n",
       "        There are several known issues with 'english' and you should\n",
       "        consider an alternative (see :ref:`stop_words`).\n",
       "\n",
       "        If a list, that list is assumed to contain stop words, all of which\n",
       "        will be removed from the resulting tokens.\n",
       "        Only applies if ``analyzer == 'word'``.\n",
       "\n",
       "        If None, no stop words will be used. max_df can be set to a value\n",
       "        in the range [0.7, 1.0) to automatically detect and filter stop\n",
       "        words based on intra corpus document frequency of terms.\n",
       "\n",
       "    token_pattern : string\n",
       "        Regular expression denoting what constitutes a \"token\", only used\n",
       "        if ``analyzer == 'word'``. The default regexp select tokens of 2\n",
       "        or more alphanumeric characters (punctuation is completely ignored\n",
       "        and always treated as a token separator).\n",
       "\n",
       "    ngram_range : tuple (min_n, max_n), default=(1, 1)\n",
       "        The lower and upper boundary of the range of n-values for different\n",
       "        word n-grams or char n-grams to be extracted. All values of n such\n",
       "        such that min_n <= n <= max_n will be used. For example an\n",
       "        ``ngram_range`` of ``(1, 1)`` means only unigrams, ``(1, 2)`` means\n",
       "        unigrams and bigrams, and ``(2, 2)`` means only bigrams.\n",
       "        Only applies if ``analyzer is not callable``.\n",
       "\n",
       "    analyzer : string, {'word', 'char', 'char_wb'} or callable, \\\n",
       "            default='word'\n",
       "        Whether the feature should be made of word n-gram or character\n",
       "        n-grams.\n",
       "        Option 'char_wb' creates character n-grams only from text inside\n",
       "        word boundaries; n-grams at the edges of words are padded with space.\n",
       "\n",
       "        If a callable is passed it is used to extract the sequence of features\n",
       "        out of the raw, unprocessed input.\n",
       "\n",
       "        .. versionchanged:: 0.21\n",
       "\n",
       "        Since v0.21, if ``input`` is ``filename`` or ``file``, the data is\n",
       "        first read from the file and then passed to the given callable\n",
       "        analyzer.\n",
       "\n",
       "    max_df : float in range [0.0, 1.0] or int, default=1.0\n",
       "        When building the vocabulary ignore terms that have a document\n",
       "        frequency strictly higher than the given threshold (corpus-specific\n",
       "        stop words).\n",
       "        If float, the parameter represents a proportion of documents, integer\n",
       "        absolute counts.\n",
       "        This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "    min_df : float in range [0.0, 1.0] or int, default=1\n",
       "        When building the vocabulary ignore terms that have a document\n",
       "        frequency strictly lower than the given threshold. This value is also\n",
       "        called cut-off in the literature.\n",
       "        If float, the parameter represents a proportion of documents, integer\n",
       "        absolute counts.\n",
       "        This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "    max_features : int, default=None\n",
       "        If not None, build a vocabulary that only consider the top\n",
       "        max_features ordered by term frequency across the corpus.\n",
       "\n",
       "        This parameter is ignored if vocabulary is not None.\n",
       "\n",
       "    vocabulary : Mapping or iterable, default=None\n",
       "        Either a Mapping (e.g., a dict) where keys are terms and values are\n",
       "        indices in the feature matrix, or an iterable over terms. If not\n",
       "        given, a vocabulary is determined from the input documents. Indices\n",
       "        in the mapping should not be repeated and should not have any gap\n",
       "        between 0 and the largest index.\n",
       "\n",
       "    binary : bool, default=False\n",
       "        If True, all non zero counts are set to 1. This is useful for discrete\n",
       "        probabilistic models that model binary events rather than integer\n",
       "        counts.\n",
       "\n",
       "    dtype : type, default=np.int64\n",
       "        Type of the matrix returned by fit_transform() or transform().\n",
       "\n",
       "    Attributes\n",
       "    ----------\n",
       "    vocabulary_ : dict\n",
       "        A mapping of terms to feature indices.\n",
       "\n",
       "    fixed_vocabulary_: boolean\n",
       "        True if a fixed vocabulary of term to indices mapping\n",
       "        is provided by the user\n",
       "\n",
       "    stop_words_ : set\n",
       "        Terms that were ignored because they either:\n",
       "\n",
       "          - occurred in too many documents (`max_df`)\n",
       "          - occurred in too few documents (`min_df`)\n",
       "          - were cut off by feature selection (`max_features`).\n",
       "\n",
       "        This is only available if no vocabulary was given.\n",
       "\n",
       "    Examples\n",
       "    --------\n",
       "    >>> from sklearn.feature_extraction.text import CountVectorizer\n",
       "    >>> corpus = [\n",
       "    ...     'This is the first document.',\n",
       "    ...     'This document is the second document.',\n",
       "    ...     'And this is the third one.',\n",
       "    ...     'Is this the first document?',\n",
       "    ... ]\n",
       "    >>> vectorizer = CountVectorizer()\n",
       "    >>> X = vectorizer.fit_transform(corpus)\n",
       "    >>> print(vectorizer.get_feature_names())\n",
       "    ['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n",
       "    >>> print(X.toarray())\n",
       "    [[0 1 1 1 0 0 1 0 1]\n",
       "     [0 2 0 1 0 1 1 0 1]\n",
       "     [1 0 0 1 1 0 1 1 1]\n",
       "     [0 1 1 1 0 0 1 0 1]]\n",
       "    >>> vectorizer2 = CountVectorizer(analyzer='word', ngram_range=(2, 2))\n",
       "    >>> X2 = vectorizer2.fit_transform(corpus)\n",
       "    >>> print(vectorizer2.get_feature_names())\n",
       "    ['and this', 'document is', 'first document', 'is the', 'is this',\n",
       "    'second document', 'the first', 'the second', 'the third', 'third one',\n",
       "     'this document', 'this is', 'this the']\n",
       "     >>> print(X2.toarray())\n",
       "     [[0 0 1 1 0 0 1 0 0 0 0 1 0]\n",
       "     [0 1 0 1 0 1 0 1 0 0 1 0 0]\n",
       "     [1 0 0 1 0 0 0 0 1 1 0 1 0]\n",
       "     [0 0 1 0 1 0 1 0 0 0 0 0 1]]\n",
       "\n",
       "    See Also\n",
       "    --------\n",
       "    HashingVectorizer, TfidfVectorizer\n",
       "\n",
       "    Notes\n",
       "    -----\n",
       "    The ``stop_words_`` attribute can get large and increase the model size\n",
       "    when pickling. This attribute is provided only for introspection and can\n",
       "    be safely removed using delattr or set to None before pickling.\n",
       "    \"\"\"\n",
       "    @_deprecate_positional_args\n",
       "    def __init__(self, *, input='content', encoding='utf-8',\n",
       "                 decode_error='strict', strip_accents=None,\n",
       "                 lowercase=True, preprocessor=None, tokenizer=None,\n",
       "                 stop_words=None, token_pattern=r\"(?u)\\b\\w\\w+\\b\",\n",
       "                 ngram_range=(1, 1), analyzer='word',\n",
       "                 max_df=1.0, min_df=1, max_features=None,\n",
       "                 vocabulary=None, binary=False, dtype=np.int64):\n",
       "        self.input = input\n",
       "        self.encoding = encoding\n",
       "        self.decode_error = decode_error\n",
       "        self.strip_accents = strip_accents\n",
       "        self.preprocessor = preprocessor\n",
       "        self.tokenizer = tokenizer\n",
       "        self.analyzer = analyzer\n",
       "        self.lowercase = lowercase\n",
       "        self.token_pattern = token_pattern\n",
       "        self.stop_words = stop_words\n",
       "        self.max_df = max_df\n",
       "        self.min_df = min_df\n",
       "        if max_df < 0 or min_df < 0:\n",
       "            raise ValueError(\"negative value for max_df or min_df\")\n",
       "        self.max_features = max_features\n",
       "        if max_features is not None:\n",
       "            if (not isinstance(max_features, numbers.Integral) or\n",
       "                    max_features <= 0):\n",
       "                raise ValueError(\n",
       "                    \"max_features=%r, neither a positive integer nor None\"\n",
       "                    % max_features)\n",
       "        self.ngram_range = ngram_range\n",
       "        self.vocabulary = vocabulary\n",
       "        self.binary = binary\n",
       "        self.dtype = dtype\n",
       "\n",
       "    def _sort_features(self, X, vocabulary):\n",
       "        \"\"\"Sort features by name\n",
       "\n",
       "        Returns a reordered matrix and modifies the vocabulary in place\n",
       "        \"\"\"\n",
       "        sorted_features = sorted(vocabulary.items())\n",
       "        map_index = np.empty(len(sorted_features), dtype=X.indices.dtype)\n",
       "        for new_val, (term, old_val) in enumerate(sorted_features):\n",
       "            vocabulary[term] = new_val\n",
       "            map_index[old_val] = new_val\n",
       "\n",
       "        X.indices = map_index.take(X.indices, mode='clip')\n",
       "        return X\n",
       "\n",
       "    def _limit_features(self, X, vocabulary, high=None, low=None,\n",
       "                        limit=None):\n",
       "        \"\"\"Remove too rare or too common features.\n",
       "\n",
       "        Prune features that are non zero in more samples than high or less\n",
       "        documents than low, modifying the vocabulary, and restricting it to\n",
       "        at most the limit most frequent.\n",
       "\n",
       "        This does not prune samples with zero features.\n",
       "        \"\"\"\n",
       "        if high is None and low is None and limit is None:\n",
       "            return X, set()\n",
       "\n",
       "        # Calculate a mask based on document frequencies\n",
       "        dfs = _document_frequency(X)\n",
       "        mask = np.ones(len(dfs), dtype=bool)\n",
       "        if high is not None:\n",
       "            mask &= dfs <= high\n",
       "        if low is not None:\n",
       "            mask &= dfs >= low\n",
       "        if limit is not None and mask.sum() > limit:\n",
       "            tfs = np.asarray(X.sum(axis=0)).ravel()\n",
       "            mask_inds = (-tfs[mask]).argsort()[:limit]\n",
       "            new_mask = np.zeros(len(dfs), dtype=bool)\n",
       "            new_mask[np.where(mask)[0][mask_inds]] = True\n",
       "            mask = new_mask\n",
       "\n",
       "        new_indices = np.cumsum(mask) - 1  # maps old indices to new\n",
       "        removed_terms = set()\n",
       "        for term, old_index in list(vocabulary.items()):\n",
       "            if mask[old_index]:\n",
       "                vocabulary[term] = new_indices[old_index]\n",
       "            else:\n",
       "                del vocabulary[term]\n",
       "                removed_terms.add(term)\n",
       "        kept_indices = np.where(mask)[0]\n",
       "        if len(kept_indices) == 0:\n",
       "            raise ValueError(\"After pruning, no terms remain. Try a lower\"\n",
       "                             \" min_df or a higher max_df.\")\n",
       "        return X[:, kept_indices], removed_terms\n",
       "\n",
       "    def _count_vocab(self, raw_documents, fixed_vocab):\n",
       "        \"\"\"Create sparse feature matrix, and vocabulary where fixed_vocab=False\n",
       "        \"\"\"\n",
       "        if fixed_vocab:\n",
       "            vocabulary = self.vocabulary_\n",
       "        else:\n",
       "            # Add a new value when a new vocabulary item is seen\n",
       "            vocabulary = defaultdict()\n",
       "            vocabulary.default_factory = vocabulary.__len__\n",
       "\n",
       "        analyze = self.build_analyzer()\n",
       "        j_indices = []\n",
       "        indptr = []\n",
       "\n",
       "        values = _make_int_array()\n",
       "        indptr.append(0)\n",
       "        for doc in raw_documents:\n",
       "            feature_counter = {}\n",
       "            for feature in analyze(doc):\n",
       "                try:\n",
       "                    feature_idx = vocabulary[feature]\n",
       "                    if feature_idx not in feature_counter:\n",
       "                        feature_counter[feature_idx] = 1\n",
       "                    else:\n",
       "                        feature_counter[feature_idx] += 1\n",
       "                except KeyError:\n",
       "                    # Ignore out-of-vocabulary items for fixed_vocab=True\n",
       "                    continue\n",
       "\n",
       "            j_indices.extend(feature_counter.keys())\n",
       "            values.extend(feature_counter.values())\n",
       "            indptr.append(len(j_indices))\n",
       "\n",
       "        if not fixed_vocab:\n",
       "            # disable defaultdict behaviour\n",
       "            vocabulary = dict(vocabulary)\n",
       "            if not vocabulary:\n",
       "                raise ValueError(\"empty vocabulary; perhaps the documents only\"\n",
       "                                 \" contain stop words\")\n",
       "\n",
       "        if indptr[-1] > np.iinfo(np.int32).max:  # = 2**31 - 1\n",
       "            if _IS_32BIT:\n",
       "                raise ValueError(('sparse CSR array has {} non-zero '\n",
       "                                  'elements and requires 64 bit indexing, '\n",
       "                                  'which is unsupported with 32 bit Python.')\n",
       "                                 .format(indptr[-1]))\n",
       "            indices_dtype = np.int64\n",
       "\n",
       "        else:\n",
       "            indices_dtype = np.int32\n",
       "        j_indices = np.asarray(j_indices, dtype=indices_dtype)\n",
       "        indptr = np.asarray(indptr, dtype=indices_dtype)\n",
       "        values = np.frombuffer(values, dtype=np.intc)\n",
       "\n",
       "        X = sp.csr_matrix((values, j_indices, indptr),\n",
       "                          shape=(len(indptr) - 1, len(vocabulary)),\n",
       "                          dtype=self.dtype)\n",
       "        X.sort_indices()\n",
       "        return vocabulary, X\n",
       "\n",
       "    def fit(self, raw_documents, y=None):\n",
       "        \"\"\"Learn a vocabulary dictionary of all tokens in the raw documents.\n",
       "\n",
       "        Parameters\n",
       "        ----------\n",
       "        raw_documents : iterable\n",
       "            An iterable which yields either str, unicode or file objects.\n",
       "\n",
       "        Returns\n",
       "        -------\n",
       "        self\n",
       "        \"\"\"\n",
       "        self._warn_for_unused_params()\n",
       "        self.fit_transform(raw_documents)\n",
       "        return self\n",
       "\n",
       "    def fit_transform(self, raw_documents, y=None):\n",
       "        \"\"\"Learn the vocabulary dictionary and return document-term matrix.\n",
       "\n",
       "        This is equivalent to fit followed by transform, but more efficiently\n",
       "        implemented.\n",
       "\n",
       "        Parameters\n",
       "        ----------\n",
       "        raw_documents : iterable\n",
       "            An iterable which yields either str, unicode or file objects.\n",
       "\n",
       "        Returns\n",
       "        -------\n",
       "        X : array of shape (n_samples, n_features)\n",
       "            Document-term matrix.\n",
       "        \"\"\"\n",
       "        # We intentionally don't call the transform method to make\n",
       "        # fit_transform overridable without unwanted side effects in\n",
       "        # TfidfVectorizer.\n",
       "        if isinstance(raw_documents, str):\n",
       "            raise ValueError(\n",
       "                \"Iterable over raw text documents expected, \"\n",
       "                \"string object received.\")\n",
       "\n",
       "        self._validate_params()\n",
       "        self._validate_vocabulary()\n",
       "        max_df = self.max_df\n",
       "        min_df = self.min_df\n",
       "        max_features = self.max_features\n",
       "\n",
       "        vocabulary, X = self._count_vocab(raw_documents,\n",
       "                                          self.fixed_vocabulary_)\n",
       "\n",
       "        if self.binary:\n",
       "            X.data.fill(1)\n",
       "\n",
       "        if not self.fixed_vocabulary_:\n",
       "            n_doc = X.shape[0]\n",
       "            max_doc_count = (max_df\n",
       "                             if isinstance(max_df, numbers.Integral)\n",
       "                             else max_df * n_doc)\n",
       "            min_doc_count = (min_df\n",
       "                             if isinstance(min_df, numbers.Integral)\n",
       "                             else min_df * n_doc)\n",
       "            if max_doc_count < min_doc_count:\n",
       "                raise ValueError(\n",
       "                    \"max_df corresponds to < documents than min_df\")\n",
       "            if max_features is not None:\n",
       "                X = self._sort_features(X, vocabulary)\n",
       "            X, self.stop_words_ = self._limit_features(X, vocabulary,\n",
       "                                                       max_doc_count,\n",
       "                                                       min_doc_count,\n",
       "                                                       max_features)\n",
       "            if max_features is None:\n",
       "                X = self._sort_features(X, vocabulary)\n",
       "            self.vocabulary_ = vocabulary\n",
       "\n",
       "        return X\n",
       "\n",
       "    def transform(self, raw_documents):\n",
       "        \"\"\"Transform documents to document-term matrix.\n",
       "\n",
       "        Extract token counts out of raw text documents using the vocabulary\n",
       "        fitted with fit or the one provided to the constructor.\n",
       "\n",
       "        Parameters\n",
       "        ----------\n",
       "        raw_documents : iterable\n",
       "            An iterable which yields either str, unicode or file objects.\n",
       "\n",
       "        Returns\n",
       "        -------\n",
       "        X : sparse matrix of shape (n_samples, n_features)\n",
       "            Document-term matrix.\n",
       "        \"\"\"\n",
       "        if isinstance(raw_documents, str):\n",
       "            raise ValueError(\n",
       "                \"Iterable over raw text documents expected, \"\n",
       "                \"string object received.\")\n",
       "        self._check_vocabulary()\n",
       "\n",
       "        # use the same matrix-building strategy as fit_transform\n",
       "        _, X = self._count_vocab(raw_documents, fixed_vocab=True)\n",
       "        if self.binary:\n",
       "            X.data.fill(1)\n",
       "        return X\n",
       "\n",
       "    def inverse_transform(self, X):\n",
       "        \"\"\"Return terms per document with nonzero entries in X.\n",
       "\n",
       "        Parameters\n",
       "        ----------\n",
       "        X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
       "            Document-term matrix.\n",
       "\n",
       "        Returns\n",
       "        -------\n",
       "        X_inv : list of arrays of shape (n_samples,)\n",
       "            List of arrays of terms.\n",
       "        \"\"\"\n",
       "        self._check_vocabulary()\n",
       "\n",
       "        if sp.issparse(X):\n",
       "            # We need CSR format for fast row manipulations.\n",
       "            X = X.tocsr()\n",
       "        else:\n",
       "            # We need to convert X to a matrix, so that the indexing\n",
       "            # returns 2D objects\n",
       "            X = np.asmatrix(X)\n",
       "        n_samples = X.shape[0]\n",
       "\n",
       "        terms = np.array(list(self.vocabulary_.keys()))\n",
       "        indices = np.array(list(self.vocabulary_.values()))\n",
       "        inverse_vocabulary = terms[np.argsort(indices)]\n",
       "\n",
       "        return [inverse_vocabulary[X[i, :].nonzero()[1]].ravel()\n",
       "                for i in range(n_samples)]\n",
       "\n",
       "    def get_feature_names(self):\n",
       "        \"\"\"Array mapping from feature integer indices to feature name.\n",
       "\n",
       "        Returns\n",
       "        -------\n",
       "        feature_names : list\n",
       "            A list of feature names.\n",
       "        \"\"\"\n",
       "\n",
       "        self._check_vocabulary()\n",
       "\n",
       "        return [t for t, i in sorted(self.vocabulary_.items(),\n",
       "                                     key=itemgetter(1))]\n",
       "\n",
       "    def _more_tags(self):\n",
       "        return {'X_types': ['string']}\n",
       "\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_object(CountVectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JobDash",
   "language": "python",
   "name": "jobdash"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
